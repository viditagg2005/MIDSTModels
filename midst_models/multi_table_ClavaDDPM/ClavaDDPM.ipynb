{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models\n",
    "\n",
    "Recent tabular data synthesis research has focused on single tables, while real-world applications often involve complex, interconnected tables. Existing methods for multi-relational data synthesis struggle with scalability and long-range dependencies. This paper introduces Cluster Latent Variable guided Denoising Diffusion Probabilistic Models (ClavaDDPM), using clustering labels to model inter-table relationships, particularly foreign key constraints. ClavaDDPM efficiently propagates latent variables across tables, capturing long-range dependencies. Evaluations show ClavaDDPM outperforms existing methods on multi-table data and remains competitive for single-table data.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Load Configuration]()\n",
    "\n",
    "\n",
    "3. [Data Loading and Preprocessing]()\n",
    "    \n",
    "    \n",
    "4. [ClavaDDPM Algorithm]()\n",
    "\n",
    "    4.1. [Overview]()\n",
    "    \n",
    "    4.2. [Clustring]()\n",
    "    \n",
    "    4.3. [Model Training]()\n",
    "    \n",
    "    4.4. [Model Sampling]()\n",
    "    \n",
    "    \n",
    "    \n",
    "6. [Model Evaluation]()\n",
    "\n",
    "    6.1. [Multi-Table Metrics]()\n",
    "    \n",
    "    6.2. [Single-Table Metrics]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_ClavaDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    clava_eval,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_ClavaDDPM.pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_ClavaDDPM.report_utils import get_multi_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configuration\n",
    "\n",
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary. We print out the entire configuration file in the code cell below and will explain the hyperparameters in more detail further down to clarify.\n",
    "\n",
    "A sample configuration file is available at `configs/trans.json`, where general parameters can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/fs01/projects/diffusion_bootcamp/data/tabular/raw_data/movie_lens/preprocessed/train\",\n",
      "        \"exp_name\": \"movie_lens_train\",\n",
      "        \"workspace_dir\": \"clavaDDPM_workspace/movie_lens\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/fs01/projects/diffusion_bootcamp/data/tabular/raw_data/movie_lens/preprocessed/test\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = \"configs/trans.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. We demonstrate the dataset's metadata and parent-child relationships to provide a clearer understanding of its structure. Following this, we perform clustering to preprocess the data, facilitating the training process for the ClavaDDPM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use all the tables from the Berka dataset. You can access the Berka dataset files here????? \n",
    "\n",
    "The BERKA dataset is a comprehensive banking dataset originally released by the Czech bank ČSOB for the Financial Modeling and Analysis (FMA) competition in 1999. It provides detailed financial data on transactions, accounts, loans, credit cards, and demographic information for thousands of customers over multiple years.\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. \n",
    "The following files are needed to be present in the data directory:\n",
    "- `{table}.csv`: The train susbet from the Berka dataset for all the tables.\n",
    "- `test.csv`: The transactions susbet from the Berka dataset used for evaluation.\n",
    "- `{table}_domain.json`: The domain file for every table.\n",
    "- `{table}_label_encoders.pkl`: The label encoders used to encode the table if you are using the already preprocessed data from here?????\n",
    "- `dataset_meta.json`: The configuration file defines the relationships between different tables in the dataset. A sample configuration file is available at `configs/dataset_meta.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table name: actor, Total dataframe shape: (81895, 2), Numerical data shape: (81895, 1), Categorical data shape: (81895, 1)\n",
      "Table name: director, Total dataframe shape: (2041, 2), Numerical data shape: (2041, 2), Categorical data shape: (2041, 0)\n",
      "Table name: user, Total dataframe shape: (6039, 3), Numerical data shape: (6039, 1), Categorical data shape: (6039, 2)\n",
      "Table name: movie, Total dataframe shape: (3650, 4), Numerical data shape: (3650, 2), Categorical data shape: (3650, 2)\n",
      "Table name: movie2actor, Total dataframe shape: (134122, 1), Numerical data shape: (134122, 1), Categorical data shape: (134122, 0)\n",
      "Table name: movie2director, Total dataframe shape: (3943, 1), Numerical data shape: (3943, 0), Categorical data shape: (3943, 1)\n",
      "Table name: rating, Total dataframe shape: (896543, 1), Numerical data shape: (896543, 1), Categorical data shape: (896543, 0)\n",
      "\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "['actor', 'director', 'user', 'movie', 'movie2actor', 'movie2director', 'rating']\n",
      "\n",
      "==================== We show the relation order below ====================\n",
      "Relation 0: None ---> actor\n",
      "Relation 1: None ---> director\n",
      "Relation 2: None ---> movie\n",
      "Relation 3: None ---> user\n",
      "Relation 4: actor ---> movie2actor\n",
      "Relation 5: movie ---> movie2actor\n",
      "Relation 6: movie ---> movie2director\n",
      "Relation 7: director ---> movie2director\n",
      "Relation 8: movie ---> rating\n",
      "Relation 9: user ---> rating\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"971pt\" height=\"334pt\"\n",
       " viewBox=\"0.00 0.00 970.50 334.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 330)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-330 966.5,-330 966.5,4 -4,4\"/>\n",
       "<!-- actor -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>actor</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M12,-211.5C12,-211.5 125,-211.5 125,-211.5 131,-211.5 137,-217.5 137,-223.5 137,-223.5 137,-298.5 137,-298.5 137,-304.5 131,-310.5 125,-310.5 125,-310.5 12,-310.5 12,-310.5 6,-310.5 0,-304.5 0,-298.5 0,-298.5 0,-223.5 0,-223.5 0,-217.5 6,-211.5 12,-211.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-295.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">actor</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"0,-287.5 137,-287.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-272.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">actor_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a_gender : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-242.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a_quality : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"0,-234.5 137,-234.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-219.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: actor_id</text>\n",
       "</g>\n",
       "<!-- movie2actor -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>movie2actor</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M97.5,-.5C97.5,-.5 261.5,-.5 261.5,-.5 267.5,-.5 273.5,-6.5 273.5,-12.5 273.5,-12.5 273.5,-132.5 273.5,-132.5 273.5,-138.5 267.5,-144.5 261.5,-144.5 261.5,-144.5 97.5,-144.5 97.5,-144.5 91.5,-144.5 85.5,-138.5 85.5,-132.5 85.5,-132.5 85.5,-12.5 85.5,-12.5 85.5,-6.5 91.5,-.5 97.5,-.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-129.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie2actor</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"85.5,-121.5 273.5,-121.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-106.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie2actor_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-76.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">actor_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-61.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cast_num : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"85.5,-53.5 273.5,-53.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-38.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: movie2actor_id</text>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-23.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (actor): actor_id</text>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (movie): movie_id</text>\n",
       "</g>\n",
       "<!-- actor&#45;&gt;movie2actor -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>actor&#45;&gt;movie2actor</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M74.7803,-211.2527C78.1938,-195.1758 83.4425,-177.7156 91.5,-163 93.3172,-159.6812 95.2896,-156.3983 97.3879,-153.1618\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"106.0979,-146.6964 97.5396,-152.9419 100.3352,-142.7225 106.0979,-146.6964\"/>\n",
       "<text text-anchor=\"middle\" x=\"150.5\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;actor_id → actor_id</text>\n",
       "</g>\n",
       "<!-- director -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>director</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M167.5,-211.5C167.5,-211.5 295.5,-211.5 295.5,-211.5 301.5,-211.5 307.5,-217.5 307.5,-223.5 307.5,-223.5 307.5,-298.5 307.5,-298.5 307.5,-304.5 301.5,-310.5 295.5,-310.5 295.5,-310.5 167.5,-310.5 167.5,-310.5 161.5,-310.5 155.5,-304.5 155.5,-298.5 155.5,-298.5 155.5,-223.5 155.5,-223.5 155.5,-217.5 161.5,-211.5 167.5,-211.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"231.5\" y=\"-295.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">director</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"155.5,-287.5 307.5,-287.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"163.5\" y=\"-272.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">director_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"163.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">d_quality : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"163.5\" y=\"-242.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">avg_revenue : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"155.5,-234.5 307.5,-234.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"163.5\" y=\"-219.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: director_id</text>\n",
       "</g>\n",
       "<!-- movie2director -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>movie2director</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M413,-.5C413,-.5 592,-.5 592,-.5 598,-.5 604,-6.5 604,-12.5 604,-12.5 604,-132.5 604,-132.5 604,-138.5 598,-144.5 592,-144.5 592,-144.5 413,-144.5 413,-144.5 407,-144.5 401,-138.5 401,-132.5 401,-132.5 401,-12.5 401,-12.5 401,-6.5 407,-.5 413,-.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.5\" y=\"-129.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie2director</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"401,-121.5 604,-121.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-106.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie2director_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-76.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">director_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-61.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">genre : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"401,-53.5 604,-53.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-38.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: movie2director_id</text>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-23.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (movie): movie_id</text>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (director): director_id</text>\n",
       "</g>\n",
       "<!-- director&#45;&gt;movie2director -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>director&#45;&gt;movie2director</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M221.0002,-211.2559C220.1743,-194.5447 222.5425,-176.6917 232.5,-163 233.7936,-161.2213 317.1415,-133.4352 390.8849,-109.1165\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"401.8469,-109.1887 391.2539,-108.995 399.6557,-102.5405 401.8469,-109.1887\"/>\n",
       "<text text-anchor=\"middle\" x=\"306.5\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;director_id → director_id</text>\n",
       "</g>\n",
       "<!-- user -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>user</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M809.5,-204C809.5,-204 931.5,-204 931.5,-204 937.5,-204 943.5,-210 943.5,-216 943.5,-216 943.5,-306 943.5,-306 943.5,-312 937.5,-318 931.5,-318 931.5,-318 809.5,-318 809.5,-318 803.5,-318 797.5,-312 797.5,-306 797.5,-306 797.5,-216 797.5,-216 797.5,-210 803.5,-204 809.5,-204\"/>\n",
       "<text text-anchor=\"middle\" x=\"870.5\" y=\"-302.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">user</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"797.5,-295 943.5,-295 \"/>\n",
       "<text text-anchor=\"start\" x=\"805.5\" y=\"-279.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">user_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"805.5\" y=\"-264.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">age : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"805.5\" y=\"-249.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">u_gender : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"805.5\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">occupation : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"797.5,-227 943.5,-227 \"/>\n",
       "<text text-anchor=\"start\" x=\"805.5\" y=\"-211.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: user_id</text>\n",
       "</g>\n",
       "<!-- rating -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>rating</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M688.5,-.5C688.5,-.5 852.5,-.5 852.5,-.5 858.5,-.5 864.5,-6.5 864.5,-12.5 864.5,-12.5 864.5,-132.5 864.5,-132.5 864.5,-138.5 858.5,-144.5 852.5,-144.5 852.5,-144.5 688.5,-144.5 688.5,-144.5 682.5,-144.5 676.5,-138.5 676.5,-132.5 676.5,-132.5 676.5,-12.5 676.5,-12.5 676.5,-6.5 682.5,-.5 688.5,-.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"770.5\" y=\"-129.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">rating</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"676.5,-121.5 864.5,-121.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-106.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">rating_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">user_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-76.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-61.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">rating : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"676.5,-53.5 864.5,-53.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-38.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: rating_id</text>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-23.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (movie): movie_id</text>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (user): user_id</text>\n",
       "</g>\n",
       "<!-- user&#45;&gt;rating -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>user&#45;&gt;rating</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M862.6121,-203.9133C859.3805,-190.0617 854.8474,-175.6176 848.5,-163 846.8411,-159.7023 845.0358,-156.4257 843.1126,-153.1833\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"840.7279,-142.8245 843.093,-153.152 834.8038,-146.5535 840.7279,-142.8245\"/>\n",
       "<text text-anchor=\"middle\" x=\"908\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;user_id → user_id</text>\n",
       "</g>\n",
       "<!-- movie -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>movie</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M496,-196.5C496,-196.5 621,-196.5 621,-196.5 627,-196.5 633,-202.5 633,-208.5 633,-208.5 633,-313.5 633,-313.5 633,-319.5 627,-325.5 621,-325.5 621,-325.5 496,-325.5 496,-325.5 490,-325.5 484,-319.5 484,-313.5 484,-313.5 484,-208.5 484,-208.5 484,-202.5 490,-196.5 496,-196.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-310.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"484,-302.5 633,-302.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-287.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">movie_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-272.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">year : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">isEnglish : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-242.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">country : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-227.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">runningtime : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"484,-219.5 633,-219.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-204.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: movie_id</text>\n",
       "</g>\n",
       "<!-- movie&#45;&gt;movie2actor -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>movie&#45;&gt;movie2actor</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M483.652,-225.5693C456.5158,-211.7848 426.0287,-195.1804 399.5,-178 390.4695,-172.1517 389.6964,-168.5838 380.5,-163 350.0569,-144.5159 315.1849,-127.5109 283.1272,-113.3524\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"275.1241,-106.0367 282.8866,-113.2474 272.3217,-112.4513 275.1241,-106.0367\"/>\n",
       "<text text-anchor=\"middle\" x=\"466\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;movie_id → movie_id</text>\n",
       "</g>\n",
       "<!-- movie&#45;&gt;movie2director -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>movie&#45;&gt;movie2director</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M545.3245,-196.3579C542.6634,-185.1654 539.6923,-173.6936 536.5,-163 535.6673,-160.2107 534.7914,-157.3839 533.8819,-154.5375\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"533.9985,-143.7145 533.8062,-154.3076 527.3506,-145.9067 533.9985,-143.7145\"/>\n",
       "<text text-anchor=\"middle\" x=\"607\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;movie_id → movie_id</text>\n",
       "</g>\n",
       "<!-- movie&#45;&gt;rating -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>movie&#45;&gt;rating</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M633.2724,-212.9435C648.4238,-202.0893 663.8755,-190.1599 677.5,-178 686.5555,-169.9179 695.6009,-160.9965 704.2987,-151.8665\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"713.7467,-146.9055 704.354,-151.8075 708.6397,-142.1181 713.7467,-146.9055\"/>\n",
       "<text text-anchor=\"middle\" x=\"758\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;movie_id → movie_id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fa22c1dd7f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load multi-table dataset\n",
    "# In this step, we load the multi-table dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "# We organize the multi-table dataset as a dictionary of tables, a list of relation orders, and a dictionary of dataset metadata.\n",
    "tables, relation_order, dataset_meta = load_multi_table(configs[\"general\"][\"data_dir\"])\n",
    "print(\"\")\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "print(list(tables.keys()))\n",
    "print(\"\")\n",
    "\n",
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for i, (from_item, to_item) in enumerate(relation_order):\n",
    "    print(\"Relation {}: {} ---> {}\".format(i, from_item, to_item))\n",
    "print(\"\")\n",
    "\n",
    "# Visualize the parent-child relationship within the multi-table dataset\n",
    "multi_meta = get_multi_metadata(tables, relation_order)\n",
    "multi_meta.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM Algorithm\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "<img src=\"figures/clavaDDPM.png\" alt=\"ClavaDDPM Model Pipeline\" width=\"960\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the training process for the ClavaDDPM model. The diagram above, taken from the original paper, illustrates the main steps: \n",
    "\n",
    "(a) **Latent learning and table augmentation (steps 1-2):** This step crossponds to clustering section, where we aim to augmente each table with associated clustering labels that used to capture inter-table relationships.\n",
    "\n",
    "(b) **Training (steps 3-5):** This step corresponds to the model training section, where we train separate conditional diffusion models and the cluster classifier models on each augmented table.\n",
    "\n",
    "(c) **Synthesis (steps 6-8):** This step corresponds to the model sampling section, where we sample the table size and generate data based on the parent-child constraints (i.e., relation order).\n",
    "\n",
    "We will implement and demonstrate each section below, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "To get started, this paper first introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. Specifically, [Gaussian Process Latent Variable Models (GPLVM)](https://pyro.ai/examples/gplvm.html) are used to discover low-dimensional manifolds in noisy, high-dimensional spaces. We run the clustering algorithm below to preprocess the data for training the ClavaDDPM model. Additionally, we empirically determine the distribution of table sizes in the dataset, which will be used in the later sampling process.\n",
    "\n",
    "Mathematically, let $x$ and $y$ denote the child and parent tables, respectively. We consider $k$ clusters and model the distribution of $h = (x; \\lambda y)$ with a Gaussian distribution around its corresponding centroid $c$, i.e.,\n",
    "$$\n",
    "P(h) = \\sum_{c=1}^{k} P(c) P(h \\mid c) = \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(h; \\mu_c, \\Sigma_c),\n",
    "$$\n",
    "where the coefficient $\\lambda$ is the reweighting term called the parent scale. We opt for diagonal covariance, i.e., $\\Sigma_c = \\operatorname{diag}(\\ldots, \\sigma_l^2, \\ldots)$, which, when properly optimized, immediately satisfies our assumptions that the foreign key groups are conditionally independent of their parent rows given the cluster. \n",
    "\n",
    "A summary of the important parameters for the clustering step includes:\n",
    "- `parent_scale`: reweighting coefficient $\\lambda$ for the parent table. The default value is 1.0. It is not a sensitive factor. Recommended range for tuning: 1.0 to 2.\n",
    "- `num_clusters`: the number of clustering centers $k$ for child tables. The default is 20. Too few or too many clusters may compromise performance. Recommended range for tuning: 10 to 50.\n",
    "- `clustering_method`: ClavaDDPM provides two ways to initialize GMM-based clustering. `gmm` uses `kmeans` for initialization, and the default method `both` uses `k-means++`. It is recommended to use `both`.\n",
    "\n",
    "We expect the clava_cluster function to return following outputs:\n",
    "- `tables`: containing the updated relational tables with data augmentation, where the latent variable is attached to the parent tables.\n",
    "- `all_group_lengths_prob_dicts`: which is a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "\n",
      "Clustering user -> rating\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "  Iteration 30\n",
      "  Iteration 40\n",
      "  Iteration 50\n",
      "Initialization converged.\n",
      "Average agree rate:  0.7661956378088088\n",
      "Number of cluster centers:  49\n",
      "Clustering movie -> rating\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "Initialization converged.\n",
      "Average agree rate:  0.23081099118604606\n",
      "Number of cluster centers:  41\n",
      "Clustering director -> movie2director\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "  Iteration 30\n",
      "  Iteration 40\n",
      "  Iteration 50\n",
      "Initialization converged.\n",
      "Average agree rate:  0.8878320329569717\n",
      "Number of cluster centers:  50\n",
      "Clustering movie -> movie2director\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "Initialization converged.\n",
      "Average agree rate:  0.9934564747494596\n",
      "Number of cluster centers:  50\n",
      "Clustering movie -> movie2actor\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "Initialization converged.\n",
      "Average agree rate:  1.0\n",
      "Number of cluster centers:  49\n",
      "Clustering actor -> movie2actor\n",
      "Initialization 0\n",
      "Initialization converged.\n",
      "Average agree rate:  0.8730326460632243\n",
      "Number of cluster centers:  49\n"
     ]
    }
   ],
   "source": [
    "# Display important clustering parameters\n",
    "params_clustering = configs[\"clustering\"]\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "    tables, relation_order, save_dir, configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the clustering results set, we can proceed to launch the training in PyTorch. As the dataset contains multiple tables, we train separate conditional diffusion models and cluster classifier models on each augmented table. Important parameters for the training process include:\n",
    "\n",
    "- `d_layers`: the dimension of layers in the diffusion model. \n",
    "- `num_timesteps`: the number of diffusion steps for adding noise and denoising. \n",
    "- `iterations`: the number of training iterations. The default is 10000. Recommended range for tuning: 5000 to 20000.\n",
    "- `batch_size`: the batch size for training. The default is 4096. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "d_layers: [512, 1024, 1024, 1024, 1024, 512]\n",
      "dropout: 0.0\n",
      "num_timesteps: 2000\n",
      "model_type: mlp\n",
      "iterations: 200000\n",
      "batch_size: 4096\n",
      "lr: 0.0006\n",
      "gaussian_loss_type: mse\n",
      "weight_decay: 1e-05\n",
      "scheduler: cosine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"diffusion\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training from Scratch\n",
    "The training process is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. Various callbacks are configured to monitor and save the model during training. The training process is then initiated, logging progress and completing the model's training. Finally, the trained models are saved to the specified directory and returned for further use. This process is happening in the `train_model` function, which gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the training process.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the trained models and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the relation order again, each line indicates one conditional generative model {}\".format(\n",
    "        \"=\" * 20, \"=\" * 20\n",
    "    )\n",
    ")\n",
    "for i, (from_item, to_item) in enumerate(relation_order):\n",
    "    print(\"Relation {}: {} ---> {}\".format(i, from_item, to_item))\n",
    "print(\"\")\n",
    "\n",
    "# Launch training from scratch\n",
    "models = clava_training(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models\n",
    "If the training process from scratch takes too long, please run the following command to load pre-trained models and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -> actor checkpoint found, loading...\n",
      "None -> director checkpoint found, loading...\n",
      "None -> movie checkpoint found, loading...\n",
      "None -> user checkpoint found, loading...\n",
      "actor -> movie2actor checkpoint found, loading...\n",
      "movie -> movie2actor checkpoint found, loading...\n",
      "movie -> movie2director checkpoint found, loading...\n",
      "director -> movie2director checkpoint found, loading...\n",
      "movie -> rating checkpoint found, loading...\n",
      "user -> rating checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Use the pre-trained models\n",
    "## save_dir was determined when loading the config file\n",
    "models = clava_load_pretrained(relation_order, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling\n",
    "\n",
    "To assess the trained model's performance, we generate synthetic samples and showcase the results qualitatively. We initiate the generation process by sampling the table size (i.e., the number of rows per table) and performing conditional generation to meet the parent-child constraints (i.e., relation order). Quantitative evaluations will be conducted in the next section.\n",
    "\n",
    "Important parameters for the sampling process include:\n",
    "- `batch_size`: Mini-batch size for sampling.\n",
    "- `classifier_scale` ($\\eta$): Controls the magnitude of classifier gradients during guided sampling, balancing sample quality and conditional sampling accuracy. The default value is 1.0. When $\\eta = 0$ (disabling classifier conditioning), single column densities (1-way) may improve but fail to capture long-range correlations. When $\\eta = 2$, the increased conditioning weight significantly improves the modeling of multi-hop correlations compared to $\\eta = 0$. Recommended tuning range: 0 to 2.\n",
    "\n",
    "<!-- We also conduct a matching process to determine the parent-child table relationship of the generated data. This process uses an approximate nearest neighbor search-based matching technique, providing a universal solution to the multi-parent relational synthesis problem for a child table with multiple parents.\n",
    "\n",
    "Important parameters are as follows:\n",
    "- `num_matching_clusters`: Number of clusters used in the matching process.\n",
    "- `matching_batch_size`: Mini-batch size for table matching.\n",
    "- `unique_matching`: Boolean flag indicating if the matching result should be unique.\n",
    "- `no_matching`: Boolean flag to disable the matching process. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "batch_size: 20000\n",
      "classifier_scale: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"sampling\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data from Scratch\n",
    "To generate synthetic data from scratch, we run the following code cell. This `clava_synthesizing` function gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the synthetic data.\n",
    "- `all_group_lengths_prob_dicts`: a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate.\n",
    "- `models`: the trained diffusion models.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the sampling process.\n",
    "- `sample_scale`: the scale factor for the sampling process.\n",
    "\n",
    "The synthetic data will be saved in the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables,\n",
    "    relation_order,\n",
    "    save_dir,\n",
    "    all_group_lengths_prob_dicts,\n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as some integer values are saved as strings during this process, we convert them back to integers for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == \"object\":\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Table Metrics\n",
    "In this step, we quantitatively evaluate the generated tabular data by computing metrics to determine the accuracy of the predictions, specifically assessing how closely the generated data matches the observed samples in the reference dataset.\n",
    "\n",
    "In particular, the critical multi-table metrics are as follows:\n",
    "\n",
    "1. Pair-wise column correlation (k-hop): This metric measures the correlations between columns from tables at a distance k (e.g., 0-hop for columns within the same table, 1-hop for a column and a column from its parent or child table).\n",
    "\n",
    "2. Average 2-way: This metric computes the average of all k-hop column-pair correlations, taking into account both short-range (k = 0) and longer-range (k > 0) dependencies.\n",
    "\n",
    "We use the [SDV evaluation API](https://docs.sdv.dev/sdv/multi-table-data/evaluation/data-quality) to obtain these metrics. For more details about the computation process, refer to their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating multi-table report for /fs01/home/sayromlou/test/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/multi_table_synthesis/clavaDDPM_workspace/movie_lens/movie_lens_train\n"
     ]
    }
   ],
   "source": [
    "# Multi-table Evaluation\n",
    "report = clava_eval(tables, save_dir, configs, relation_order, cleaned_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hop_relation metrics:\n",
      "                   1-hop column correlation, format '(Parent Table, Child Table, Column 1, Column 2): Correlation score'\n",
      "                     ('actor', 'movie2actor', 'a_gender', 'cast_num'): 0.8339512674305941\n",
      "                     ('actor', 'movie2actor', 'a_quality', 'cast_num'): 0.9822094956586155\n",
      "                     ('movie', 'movie2actor', 'year', 'cast_num'): 0.9491501647126974\n",
      "                     ...... other rows are omitted ......\n",
      "\n",
      "                   0-hop column correlation, format '(Parent Table, Parent Table, Column 1, Column 2): Correlation score'\n",
      "                     ('actor', 'actor', 'a_gender', 'a_quality'): 0.008217839916966896\n",
      "                     ('director', 'director', 'd_quality', 'avg_revenue'): 0.9945964590236797\n",
      "                     ('user', 'user', 'age', 'u_gender'): 0.5\n",
      "                     ...... other rows are omitted ......\n",
      "\n",
      "avg_scores metrics:\n",
      "                   1-hop column correlation: 0.6341626032753243\n",
      "                   0-hop column correlation: 0.6285458502314192\n",
      "all_avg_score       : 0.6320321107414293\n"
     ]
    }
   ],
   "source": [
    "# Print out the multi-table metrics\n",
    "n_rows = 3\n",
    "for key, val in report.items():\n",
    "    if key in [\"hop_relation\", \"avg_scores\", \"all_avg_score\"]:\n",
    "        if key == \"hop_relation\":\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                if k > 0:\n",
    "                    print(\n",
    "                        \"{:20}-hop column correlation, format '(Parent Table, Child Table, Column 1, Column 2): Correlation score'\".format(\n",
    "                            k\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"{:20}-hop column correlation, format '(Parent Table, Parent Table, Column 1, Column 2): Correlation score'\".format(\n",
    "                            k\n",
    "                        )\n",
    "                    )\n",
    "                for i_row, (k2, v2) in enumerate(v.items()):\n",
    "                    if i_row < n_rows:\n",
    "                        print(\"{:20} {}: {}\".format(\"\", k2, v2))\n",
    "                print(\"{:20} ...... other rows are omitted ......\\n\".format(\"\"))\n",
    "        elif key == \"avg_scores\":\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                print(\"{:20}-hop column correlation: {}\".format(k, v))\n",
    "        elif key == \"all_avg_score\":\n",
    "            print(\"{:20}: {}\".format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Table Metrics\n",
    "While here we study multi-table metrics, we also provide a notebook to evaluate single table metrics for each synthesized table. Please refer to `single_table_TabDDPM/evalutate_synthetic_data.ipynb` for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Prepare the synthetic data and reference data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(save_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    # uncomment and run the following line if you want to use the pre-synthesized data\n",
    "    # shutil.copy(os.path.join(pretrained_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir, verbose=False)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'], verbose=False)\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models_show",
   "language": "python",
   "name": "diffusion_models_show"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
