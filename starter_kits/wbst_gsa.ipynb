{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path \n",
    "from functools import partial\n",
    "from typing import Callable, Any,List\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from data import get_challenge_points\n",
    "from metrics import get_tpr_at_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.wb_complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.wb_pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.tab_ddpm.gaussian_multinomial_diffsuion import GaussianMultinomialDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABDDPM_DATA_DIR = \"tabddpm_white_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_white_box\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/midst_models/single_table_TabDDPM/tabddpm_test\",\n",
      "        \"exp_name\": \"train_1\",\n",
      "        \"workspace_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/midst_models/single_table_TabDDPM/tabddpm_test\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/midst_models/single_table_TabDDPM/tabddpm_test\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/trans.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -> trans checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "relation_order = [[None,'trans']]\n",
    "models = clava_load_pretrained(relation_order,save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[None,'trans']['diffusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMultinomialDiffusion(\n",
       "  (_denoise_fn): MLPDiffusion(\n",
       "    (mlp): MLP(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (linear): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Block(\n",
       "          (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (head): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "    (proj): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating funcitons for GSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing code to implement gsa model\n",
    "#### Notes\n",
    "- Can we augment gsa1 with pca so we reduce the dimensionality of the gradients so that we dont have to compromise with the accuracy as well as compute?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabddpm = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsa1_attack(\n",
    "    model: GaussianMultinomialDiffusion, \n",
    "    x0: torch.Tensor, \n",
    "    num_timesteps: int = 10\n",
    ")-> List[float]:\n",
    "    \n",
    "    model.eval()\n",
    "    timesteps = torch.linspace(0, model.num_timesteps-1, num_timesteps).long()\n",
    "    losses = []\n",
    "    \n",
    "    # Compute losses at sampled timesteps\n",
    "    for t in timesteps:\n",
    "        #xt, epsilon = forward_diffusion(x0, t, model)\n",
    "        x_t = model.gaussian_q_sample(x_start=x0, t=torch.tensor([t]))\n",
    "        loss = compute_loss(model, xt, t, epsilon)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Average losses and compute gradients\n",
    "    avg_loss = torch.mean(torch.stack(losses))\n",
    "    model.zero_grad()\n",
    "    avg_loss.backward()\n",
    "    \n",
    "    # Aggregate gradients (L2 norm per layer)\n",
    "    gradients = [param.grad.detach().norm().item() for param in model.parameters()]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1605\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3900.0\u001b[39m,\u001b[38;5;241m41832.1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2006\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m14.6\u001b[39m,\u001b[38;5;241m13224.4\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m xt \u001b[38;5;241m=\u001b[39m tabddpm\u001b[38;5;241m.\u001b[39mgaussian_q_sample(x_start \u001b[38;5;241m=\u001b[39m x, t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1995\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(xt)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# this is just a code you wrote to check the denoising output of the tabddpm model; remove it later\n",
    "x1 = torch.tensor([1605,2,4,3900.0,41832.1,1,0,0]).to(device)\n",
    "x2 = torch.tensor([2006,2,4,14.6,13224.4,6,0,0]).to(device)\n",
    "x = torch.tensor([x1,x2]).to(device)\n",
    "xt = tabddpm.gaussian_q_sample(x_start = x, t = torch.tensor([1995]).to(device))\n",
    "print(xt)\n",
    "y= {'y': 0}\n",
    "losses = tabddpm.mixed_loss(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mixed_loss() missing 1 required positional argument: 'out_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1605\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3900.0\u001b[39m,\u001b[38;5;241m41832.1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m2006\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m14.6\u001b[39m,\u001b[38;5;241m13224.4\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]}\n\u001b[0;32m----> 5\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtabddpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixed_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(losses)\n",
      "\u001b[0;31mTypeError\u001b[0m: mixed_loss() missing 1 required positional argument: 'out_dict'"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([1605,2,4,3900.0,41832.1,1,0,0]).to(device)\n",
    "x2 = torch.tensor([2006,2,4,14.6,13224.4,6,0,0]).to(device)\n",
    "x = torch.tensor([[1605,2,4,3900.0,41832.1,1,0,0],[2006,2,4,14.6,13224.4,6,0,0]]).to(device)\n",
    "y = {'y':[0,1]}\n",
    "losses = tabddpm.mixed_loss(x)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsa2_attack(\n",
    "    model: GaussianMultinomialDiffusion, \n",
    "    x0: torch.Tensor, \n",
    "    num_timesteps: int = 10\n",
    ") -> List[float]:\n",
    "    model.eval()\n",
    "    timesteps = torch.linspace(0, model.T-1, num_timesteps).long()\n",
    "    gradients_accum = None\n",
    "    \n",
    "    for t in timesteps:\n",
    "        xt, epsilon = model.gaussian_q_sample(x0, t, model)\n",
    "        loss = compute_loss(model, xt, t, epsilon)\n",
    "        \n",
    "        # Compute gradients for this timestep\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        current_gradients = [param.grad.detach().clone() for param in model.parameters()]\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        if gradients_accum is None:\n",
    "            gradients_accum = current_gradients\n",
    "        else:\n",
    "            gradients_accum = [g_prev + g_curr for g_prev, g_curr in zip(gradients_accum, current_gradients)]\n",
    "    \n",
    "    # Average gradients across timesteps\n",
    "    averaged_gradients = [g / num_timesteps for g in gradients_accum]\n",
    "    \n",
    "    # Aggregate gradients (L2 norm per layer)\n",
    "    aggregated = [g.norm().item() for g in averaged_gradients]\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained diffusion model (example)\n",
    "x0 = torch.randn(1, 3, 32, 32)  # Example input (CIFAR-10 image)\n",
    "\n",
    "# Extract features using GSA1 and GSA2\n",
    "gsa1_features = gsa1_attack(model, x0, num_timesteps=10)\n",
    "gsa2_features = gsa2_attack(model, x0, num_timesteps=10)\n",
    "\n",
    "print(\"GSA1 Features:\", gsa1_features)\n",
    "print(\"GSA2 Features:\", gsa2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset (member=1, non-member=0)\n",
    "X_train = np.array([gsa1_features, gsa2_features])  # Replace with actual features\n",
    "y_train = np.array([1, 0])\n",
    "\n",
    "# Train a classifier (e.g., Random Forest)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midst-models-qGUjvEOx-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
