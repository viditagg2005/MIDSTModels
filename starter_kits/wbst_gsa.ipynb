{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path \n",
    "from functools import partial\n",
    "from typing import Callable, Any,List\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from data import get_challenge_points\n",
    "from metrics import get_tpr_at_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.wb_complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.wb_pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.tab_ddpm.gaussian_multinomial_diffsuion import GaussianMultinomialDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABDDPM_DATA_DIR = \"tabddpm_white_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_white_box\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/midst_models/single_table_TabDDPM/tabddpm_test\",\n",
      "        \"exp_name\": \"train_1\",\n",
      "        \"workspace_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/midst_models/single_table_TabDDPM/tabddpm_test\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/midst_models/single_table_TabDDPM/tabddpm_test\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/trans.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -> trans checkpoint found, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidit/.cache/pypoetry/virtualenvs/midst-models-qGUjvEOx-py3.9/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/vidit/.cache/pypoetry/virtualenvs/midst-models-qGUjvEOx-py3.9/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator QuantileTransformer from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "relation_order = [[None,'trans']]\n",
    "models = clava_load_pretrained(relation_order,save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[None,'trans']['diffusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMultinomialDiffusion(\n",
       "  (_denoise_fn): MLPDiffusion(\n",
       "    (mlp): MLP(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (linear): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Block(\n",
       "          (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (head): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "    (proj): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating funcitons for GSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing code to implement gsa model\n",
    "#### Notes\n",
    "- Can we augment gsa1 with pca so we reduce the dimensionality of the gradients so that we dont have to compromise with the accuracy as well as compute?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabddpm = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsa1_attack(\n",
    "    model: GaussianMultinomialDiffusion, \n",
    "    x0: torch.Tensor, \n",
    "    num_timesteps: int = 10\n",
    ")-> List[float]:\n",
    "    \n",
    "    model.eval()\n",
    "    timesteps = torch.linspace(0, model.T-1, num_timesteps).long()\n",
    "    losses = []\n",
    "    \n",
    "    # Compute losses at sampled timesteps\n",
    "    for t in timesteps:\n",
    "        #xt, epsilon = forward_diffusion(x0, t, model)\n",
    "        x_t = model.gaussian_q_sample(x_start=x0, t=torch.tensor([t]))\n",
    "        loss = compute_loss(model, xt, t, epsilon)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Average losses and compute gradients\n",
    "    avg_loss = torch.mean(torch.stack(losses))\n",
    "    model.zero_grad()\n",
    "    avg_loss.backward()\n",
    "    \n",
    "    # Aggregate gradients (L2 norm per layer)\n",
    "    gradients = [param.grad.detach().norm().item() for param in model.parameters()]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4564e+00,  8.0575e-01,  5.0939e-02,  1.0560e+01,  1.3110e+02,\n",
       "         9.9643e-03, -6.6006e-01,  2.0293e-02], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is just a code you wrote to check the denoising output of the tabddpm model; remove it later\n",
    "x = torch.Tensor([1605,2,4,3900.0,41832.1,1,0,0]).to(device)\n",
    "xt = tabddpm.gaussian_q_sample(x_start = x, t = torch.tensor([1995]).to(device))\n",
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsa2_attack(\n",
    "    model: DiffusionModel, \n",
    "    x0: torch.Tensor, \n",
    "    num_timesteps: int = 10\n",
    ") -> List[float]:\n",
    "    model.eval()\n",
    "    timesteps = torch.linspace(0, model.T-1, num_timesteps).long()\n",
    "    gradients_accum = None\n",
    "    \n",
    "    for t in timesteps:\n",
    "        xt, epsilon = forward_diffusion(x0, t, model)\n",
    "        loss = compute_loss(model, xt, t, epsilon)\n",
    "        \n",
    "        # Compute gradients for this timestep\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        current_gradients = [param.grad.detach().clone() for param in model.parameters()]\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        if gradients_accum is None:\n",
    "            gradients_accum = current_gradients\n",
    "        else:\n",
    "            gradients_accum = [g_prev + g_curr for g_prev, g_curr in zip(gradients_accum, current_gradients)]\n",
    "    \n",
    "    # Average gradients across timesteps\n",
    "    averaged_gradients = [g / num_timesteps for g in gradients_accum]\n",
    "    \n",
    "    # Aggregate gradients (L2 norm per layer)\n",
    "    aggregated = [g.norm().item() for g in averaged_gradients]\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained diffusion model (example)\n",
    "model = DiffusionModel(T=1000)\n",
    "x0 = torch.randn(1, 3, 32, 32)  # Example input (CIFAR-10 image)\n",
    "\n",
    "# Extract features using GSA1 and GSA2\n",
    "gsa1_features = gsa1_attack(model, x0, num_timesteps=10)\n",
    "gsa2_features = gsa2_attack(model, x0, num_timesteps=10)\n",
    "\n",
    "print(\"GSA1 Features:\", gsa1_features)\n",
    "print(\"GSA2 Features:\", gsa2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset (member=1, non-member=0)\n",
    "X_train = np.array([gsa1_features, gsa2_features])  # Replace with actual features\n",
    "y_train = np.array([1, 0])\n",
    "\n",
    "# Train a classifier (e.g., Random Forest)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midst-models-qGUjvEOx-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
