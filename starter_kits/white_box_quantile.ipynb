{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path \n",
    "from functools import partial\n",
    "from typing import Callable, Any,List\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from data import get_challenge_points\n",
    "from metrics import get_tpr_at_fpr\n",
    "from midst_models.single_table_TabDDPM.wb_pipeline_utils import quantile_normalize_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.wb_complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.wb_pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.tab_ddpm.gaussian_multinomial_diffsuion import GaussianMultinomialDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABDDPM_DATA_DIR = \"tabddpm_white_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_white_box\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1\",\n",
      "        \"exp_name\": \"\",\n",
      "        \"workspace_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/trans.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general': {'data_dir': '/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1',\n",
       "  'exp_name': '',\n",
       "  'workspace_dir': '/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1',\n",
       "  'sample_prefix': '',\n",
       "  'test_data_dir': '/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1'},\n",
       " 'clustering': {'parent_scale': 1.0,\n",
       "  'num_clusters': 50,\n",
       "  'clustering_method': 'both'},\n",
       " 'diffusion': {'d_layers': [512, 1024, 1024, 1024, 1024, 512],\n",
       "  'dropout': 0.0,\n",
       "  'num_timesteps': 2000,\n",
       "  'model_type': 'mlp',\n",
       "  'iterations': 200000,\n",
       "  'batch_size': 4096,\n",
       "  'lr': 0.0006,\n",
       "  'gaussian_loss_type': 'mse',\n",
       "  'weight_decay': 1e-05,\n",
       "  'scheduler': 'cosine'},\n",
       " 'classifier': {'d_layers': [128, 256, 512, 1024, 512, 256, 128],\n",
       "  'lr': 0.0001,\n",
       "  'dim_t': 128,\n",
       "  'batch_size': 4096,\n",
       "  'iterations': 20000},\n",
       " 'sampling': {'batch_size': 20000, 'classifier_scale': 1.0},\n",
       " 'matching': {'num_matching_clusters': 1,\n",
       "  'matching_batch_size': 1000,\n",
       "  'unique_matching': True,\n",
       "  'no_matching': False}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -> trans checkpoint found, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidit/.cache/pypoetry/virtualenvs/midst-models-qGUjvEOx-py3.9/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/vidit/.cache/pypoetry/virtualenvs/midst-models-qGUjvEOx-py3.9/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator QuantileTransformer from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "relation_order = [[None,'trans']]\n",
    "models = clava_load_pretrained(relation_order,save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[None,'trans']['diffusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMultinomialDiffusion(\n",
       "  (_denoise_fn): MLPDiffusion(\n",
       "    (mlp): MLP(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (linear): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Block(\n",
       "          (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): Block(\n",
       "          (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (head): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "    (proj): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Functions for Quantile Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Quantile Regressor Network #\n",
    "##############################\n",
    "class QuantileRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(QuantileRegressor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # outputs the predicted threshold\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Pinball Loss (Quantile Loss)#\n",
    "##############################\n",
    "def pinball_loss(q_pred, target, alpha):\n",
    "    \"\"\"\n",
    "    Computes the pinball (quantile) loss.\n",
    "    For each sample:\n",
    "      if target <= q_pred: loss = (q_pred - target) * (1 - alpha)\n",
    "      else:                loss = (q_pred - target) * (-alpha)\n",
    "    \"\"\"\n",
    "    indicator = (target <= q_pred).float()\n",
    "    loss = (q_pred - target) * (indicator - alpha)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Training Loop for a Model  #\n",
    "##############################\n",
    "def train_model(model, dataloader, optimizer, alpha, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            q_pred = model(X_batch)  # predicted quantile threshold\n",
    "            loss = pinball_loss(q_pred, y_batch, alpha)\n",
    "            #loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "            optimizer.zero_grad()\n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_t_error(model, x, t):\n",
    "    \"\"\"\n",
    "    Computes the reconstruction error at a specific timestep t.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained GaussianMultinomialDiffusion model.\n",
    "        x: Input data tensor.\n",
    "        t: Diffusion timestep (int or tensor).\n",
    "\n",
    "    Returns:\n",
    "        Reconstruction error tensor for each sample in x.\n",
    "    \"\"\"\n",
    "    # l = [2.1900000e+03, 2.0000000e+00, 5.0000000e+00, 8.0100000e+04,\n",
    "    #    1.4015810e+05, 8.0000000e+00, 1.3000000e+01, 9.7259157e+07] # this list was supposed to be the max values in each coulmn of challenge_with_id\n",
    "\n",
    "    #l = torch.tensor(l).to(device=\"cuda\")\n",
    "    # Simulate the diffusion process\n",
    "    #x = x/l \n",
    "    x_t = model.gaussian_q_sample(x_start=x, t=torch.tensor([t]).to(x.device))\n",
    "    \n",
    "    #x_t = x_t/l\n",
    "    # Perform reverse denoising\n",
    "    x_recon = model._denoise_fn(x_t, torch.tensor([t]).to(x.device))\n",
    "    \n",
    "    # Compute reconstruction error (e.g., L2 norm)\n",
    "    t_error = torch.norm(x - x_recon, p=2, dim=1)\n",
    "    return t_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_membership_inference(models, x, threshold_alpha):\n",
    "    \"\"\"\n",
    "    Perform membership inference using reconstruction errors and a quantile regression model.\n",
    "    \"\"\"\n",
    "    errors = compute_t_error(model, x, t=5)  # Example timestep\n",
    "    thresholds = models(x)\n",
    "    predictions = (errors <= thresholds).float()  # Membership decision\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Quantile Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfirst write the code for getting normalized train_with_id data and then do the same for holdout_with_id then train two quantile regressor models, one on holdout error\\nand the other on train error, then if the error on the cp is less than holdout error then it is train point, or otherwise you can train the two quantile models and check\\nwhich distribution has the higher probability of containing the data point.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "first write the code for getting normalized train_with_id data and then do the same for holdout_with_id then train two quantile regressor models, one on holdout error\n",
    "and the other on train error, then if the error on the cp is less than holdout error then it is train point, or otherwise you can train the two quantile models and check\n",
    "which distribution has the higher probability of containing the data point.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading & Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "def load_csv_as_tensor(file_path, nrows=None):\n",
    "    df = pd.read_csv(file_path, header=None, nrows=nrows)  # Read without headers\n",
    "    df = df.iloc[1:, 2:]  # Drop first two columns\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')  # Convert all values to numeric\n",
    "    df = df.fillna(0)  # Replace NaNs with 0 (or use another strategy)\n",
    "    return torch.tensor(df.values, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# Load and process the first CSV file\n",
    "file1 = \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/train_with_id.csv\"\n",
    "tensor1 = load_csv_as_tensor(file1)\n",
    "\n",
    "# Load and process the second CSV file (only first 20,000 rows)\n",
    "file2 = \"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/holdout_with_id.csv\"  \n",
    "tensor2 = load_csv_as_tensor(file2, nrows=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 8]) torch.Size([19999, 8])\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.shape, tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1_nor, transformers_train = quantile_normalize_sklearn(tensor1.cpu().numpy())\n",
    "tensor2_nor, transformers_holdout = quantile_normalize_sklearn(tensor1.cpu().numpy())\n",
    "\n",
    "final_tensor1_nor = torch.tensor(tensor1_nor,dtype = torch.float32).to(device)\n",
    "final_tensor2_nor = torch.tensor(tensor2_nor,dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 8])\n"
     ]
    }
   ],
   "source": [
    "print(final_tensor1_nor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = compute_t_error(model, final_tensor1_nor, t= 100)\n",
    "error_holdout = compute_t_error(model,final_tensor2_nor,t = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Single Quantile Regressor on All Training Data\n",
      "Epoch   0: Loss = 2.1484\n",
      "Epoch  20: Loss = 2.1484\n"
     ]
    }
   ],
   "source": [
    "# training first quantile regressor on train_with_id\n",
    "\n",
    "input_dim = 8\n",
    "X_train  = final_tensor1_nor\n",
    "y_train = error_train.to(device)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "alpha = 0.1        # target quantile level (e.g., 0.1 gives the 10% quantile threshold)\n",
    "num_epochs = 30   # epochs for training\n",
    "quantile_regressor_train = QuantileRegressor(input_dim, hidden_dim=64).to(device)\n",
    "optimizer = optim.Adam(quantile_regressor_train.parameters(), lr=1e-3)\n",
    "print(\"Training Single Quantile Regressor on All Training Data\")\n",
    "train_model(quantile_regressor_train, dataloader, optimizer, alpha, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Single Quantile Regressor on Holdout Data\n",
      "Epoch   0: Loss = 19.4127\n",
      "Epoch  20: Loss = 19.4127\n"
     ]
    }
   ],
   "source": [
    "# training quantile regressor on holdout_with_id\n",
    "\n",
    "X_train = final_tensor2_nor\n",
    "y_train = error_holdout.to(device)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "alpha = 0.9        # target quantile level (e.g., 0.1 gives the 10% quantile threshold)\n",
    "num_epochs = 30   # epochs for training\n",
    "quantile_regressor_holdout = QuantileRegressor(input_dim, hidden_dim=64).to(device)\n",
    "optimizer = optim.Adam(quantile_regressor_holdout.parameters(), lr=1e-3)\n",
    "print(\"Training Single Quantile Regressor on Holdout Data\")\n",
    "train_model(quantile_regressor_holdout, dataloader, optimizer, alpha, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data= load_csv_as_tensor(\"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/challenge_with_id.csv\").to(device='cuda')\n",
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_nor_train = np.empty((test_data.shape[0], 0))\n",
    "test_data = test_data.cpu().numpy()\n",
    "for i in range(8):\n",
    "    transformed_column = transformers_train[i].transform(test_data[:,i].reshape(-1,1))\n",
    "    test_data_nor_train = np.concatenate((test_data_nor_train, transformed_column), axis=1)\n",
    "\n",
    "final_test_data_nor_train = torch.tensor(test_data_nor_train,dtype = torch.float32).to(device)\n",
    "\n",
    "test_data_nor_holdout = np.empty((test_data.shape[0], 0))\n",
    "test_data = test_data\n",
    "for i in range(8):\n",
    "    transformed_column = transformers_holdout[i].transform(test_data[:,i].reshape(-1,1))\n",
    "    test_data_nor_holdout = np.concatenate((test_data_nor_holdout, transformed_column), axis=1)\n",
    "\n",
    "final_test_data_nor_holdout = torch.tensor(test_data_nor_holdout,dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 8]) torch.Size([200, 8])\n"
     ]
    }
   ],
   "source": [
    "print(final_test_data_nor_train.shape, final_test_data_nor_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1]) torch.Size([200, 1]) torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "pred_error_train = quantile_regressor_train(final_test_data_nor_train)\n",
    "pred_error_holdout = quantile_regressor_holdout(final_test_data_nor_holdout)\n",
    "actual_error = compute_t_error(model,final_test_data_nor_train,t = 100)\n",
    "\n",
    "print(pred_error_train.shape,pred_error_train.shape,actual_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nControl Point Predictions:\")\n",
    "print(\"Training model prediction:\", pred_error_train.item())\n",
    "print(\"Holdout model prediction: \", pred_error_holdout.item())\n",
    "\n",
    "if pred_error_train.item() < pred_error_holdout.item():\n",
    "    print(\"Control point is considered a training point (lower error).\")\n",
    "else:\n",
    "    print(\"Control point is not a training point (higher holdout error).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenge_labels = pd.read_csv(\"/home/vidit/Desktop/SaTML/MIDSTModels/starter_kits/tabddpm_white_box/train/tabddpm_1/challenge_label.csv\")\n",
    "challenge_ground_truth = challenge_labels[\"is_train\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    train_err = pred_error_train[i].item()\n",
    "    holdout_err = pred_error_holdout[i].item()\n",
    "    actual_err = actual_error[i].item()\n",
    "    classification = 0 if (abs(actual_err - train_err) > abs(actual_err - holdout_err)) else 1\n",
    "    predictions.append(classification)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5750\n",
      "TPR at 10% FPR: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve\n",
    "\n",
    "# Example inputs (replace with actual data)\n",
    "y_true = challenge_ground_truth  # Binary labels (0 or 1)\n",
    "y_pred = predictions  # Predicted scores (continuous)\n",
    "\n",
    "# Compute Accuracy\n",
    "y_pred_labels = (y_pred >= 0.5).astype(int)  # Convert scores to binary labels\n",
    "accuracy = accuracy_score(y_true, y_pred_labels)\n",
    "\n",
    "# Compute TPR at the closest FPR to 10%\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "closest_idx = np.argmin(np.abs(fpr - 0.1))  # Find the index where FPR is closest to 10%\n",
    "tpr_at_10_fpr = tpr[closest_idx]\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"TPR at 10% FPR: {tpr_at_10_fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # # =============================================================================\n",
    "    # # Inference: For each test sample, use the trained model to predict a threshold.\n",
    "    # # Compare the actual score to the threshold to decide membership:\n",
    "    # # If actual score <= predicted threshold, output 1 (member); else, output 0 (nonmember).\n",
    "    # # =============================================================================\n",
    "    # output_list = []\n",
    "    # for sample in test_tensor:\n",
    "    #     x_sample = sample[:input_dim].unsqueeze(0)  # shape: [1, input_dim]\n",
    "    #     actual_score = sample[input_dim].item()       # the observed score for this sample\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         pred_threshold = model(x_sample).item()\n",
    "    #     decision = 1 if actual_score <= pred_threshold else 0\n",
    "    #     output_list.append(decision)\n",
    "    \n",
    "    # # Create a tensor of output predictions.\n",
    "    # output = torch.tensor(output_list)\n",
    "    \n",
    "    # print(\"\\nFinal membership predictions (1 = member, 0 = nonmember):\")\n",
    "    # print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midst-models-qGUjvEOx-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
